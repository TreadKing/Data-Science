{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devang Patel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "In this homework assignment, your friends in forecast department are in trouble and you will help them implement a number of performance metrics used in evaluating their forecasts. \n",
    "\n",
    "There are many types of forecasts, each of which calls for slightly different methods of verification. Your friends are primarily interested in dichotomous forecasts. Quick research leads you to the fact that it is essentially binary prediction. \n",
    "\n",
    "Their terminology is slightly different. Following are the differences:\n",
    "- Instead of True Positives (TP) --> they use _Hits_ (a), \n",
    "- Instead of False Positives (FP) --> they use _False Alarms_ (b)\n",
    "- Instead of False Negatives (FN) --> they use _Misses_ (c), \n",
    "- Instead of True Negatives (TN) --> they use _Correct Rejects_ (d).\n",
    "\n",
    "For every single model run, you are given:\n",
    "1. a set of observations (Event [1] or No Event [0]) \n",
    "2. a set of prediction scores (a float between 0 and 1) and an event threshold, where the predicted outcome will be \n",
    "\n",
    "<center>$\\hat{y}= \n",
    "\\begin{cases}\n",
    "    1 \\text{ (Event occurred)},& \\text{if } score \\geq threshold\\\\\n",
    "    0 \\text{ (No event),}      & \\text{if } score < threshold\n",
    "\\end{cases} $ <center>\n",
    "\n",
    "Note that observations are ground truth and prediction scores and threshold will be used for determining the predicted model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Here are some test data you can use for this homework\n",
    "Y_test = [0, 0, 1, 1, 0, 1, 0, 0, 0, 1] # observations\n",
    "P_scores = [0.1, 0.32, 0.48, 0.9, 0.6, 0.55, 0.42, 0.37, 0.61, 0.66] # prediction scores\n",
    "threshold = 0.5 # prediction threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 (20 points)\n",
    "Given prediction scores, threshold, and observations create a function to determine the elements of a confusion matrix. For ease of use, you will output a `dict` (dictionary) object instead of a 2-dimensional numpy array. Note that _positive_ corresponds to the event occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'TP': 3, 'FN': 1, 'FP': 2, 'TN': 4}"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "\n",
    "def binary_conf_matrix(observation, p_scores, threshold):\n",
    "    '''Finds the entries (TP, FP, FN, TN) in a binary confusion matrix for forecast problems\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    observation: list\n",
    "        list of observations (1 is there is event, 0 is there is no event)\n",
    "    p_scores: list\n",
    "        list of prediction scores (scores vary between 0 and 1)\n",
    "    threshold: float\n",
    "        threshold that will be used for binary outcome from prediction scores\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        a dictionary that shows the counts for TP, TN, FP, and FNs.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    bcmd = {'TP':0, 'FN':0, 'FP':0, 'TN':0}\n",
    "    # your code goes here    \n",
    "\n",
    "    np_scores = np.array(p_scores)\n",
    "    scores = np.where(np_scores > threshold, 1, 0)\n",
    "    # compare = Y_test == scores\n",
    "    # print(compare)\n",
    "    # print(scores)\n",
    "\n",
    "    for i in range(len(scores)):\n",
    "        if(scores[i] == 1 and observation[i] == 1):\n",
    "            bcmd['TP'] = (bcmd['TP'] + 1)\n",
    "\n",
    "        elif(scores[i] == 1 and observation[i] == 0):\n",
    "            bcmd['FP'] = (bcmd['FP'] + 1)\n",
    "\n",
    "        elif(scores[i] == 0 and observation[i] == 1):\n",
    "            bcmd['FN'] = (bcmd['FN'] + 1)\n",
    "\n",
    "        elif(scores[i] == 0 and observation[i] == 0):\n",
    "            bcmd['TN'] = (bcmd['TN'] + 1)\n",
    "\n",
    "    return bcmd\n",
    "\n",
    "# test your code\n",
    "binary_conf_matrix( Y_test, P_scores, threshold )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next six questions will use the same input parameters that are used in `binary_conf_matrix` function. Please refer to the Q1 starter code documentation for the input parameters of the Questions 2-7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2 (20 points)\n",
    "\n",
    "Create functions for calculating accuracy, precision, recall, and F1-score. You can use the definitions from slides (Chapter 10). (You are supposed to calculate the precision and recall (and thus F1-score) for 'Event' [1] class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.7\n0.6\n0.75\n0.6666666666666665\n"
    }
   ],
   "source": [
    "def accuracy(observation, p_scores, threshold):\n",
    "    #your code goes here\n",
    "    pass\n",
    "    matrix = binary_conf_matrix(observation, p_scores, threshold)\n",
    "    acc = ((matrix['TP'] + matrix['TN']) / np.array(list(matrix.values())).sum())\n",
    "    return acc\n",
    "def precision(observation, p_scores, threshold):\n",
    "    #your code goes here\n",
    "    pass\n",
    "    matrix = binary_conf_matrix(observation, p_scores, threshold)\n",
    "    per = (matrix['TP'] / (matrix['TP'] + matrix['FP']))\n",
    "    return per\n",
    "\n",
    "def recall(observation, p_scores, threshold):\n",
    "    #your code goes here\n",
    "    pass\n",
    "    matrix = binary_conf_matrix(observation, p_scores, threshold)\n",
    "    re = (matrix['TP'] / (matrix['TP'] + matrix['FN']))\n",
    "    return re\n",
    "\n",
    "def f1score(observation, p_scores, threshold):\n",
    "    # your code goes here\n",
    "    pass\n",
    "    matrix = binary_conf_matrix(observation, p_scores, threshold)\n",
    "    pers = precision(observation, p_scores, threshold)\n",
    "    rec = recall(observation, p_scores, threshold)\n",
    "    f1 = 2*((pers*rec)/(pers+rec))\n",
    "    return f1\n",
    "\n",
    "# test your code using the following\n",
    "print(accuracy( Y_test, P_scores, threshold ))\n",
    "print(precision( Y_test, P_scores, threshold ))\n",
    "print(recall( Y_test, P_scores, threshold ))\n",
    "print(f1score( Y_test, P_scores, threshold ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3 (10 points)\n",
    "Calculate the bias score (BIAS). Bias score measures the ratio of the frequency of predicted event occurrences to the frequency of observed events. It can be calculated using the following formula:\n",
    "\n",
    "$ BIAS = \\frac{\\text{hits} + \\text{false alarms} }{ \\text{hits} + \\text{misses} }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bias score\n",
    "\n",
    "def bias_score(observation, p_scores, threshold):\n",
    "    #your code goes here\n",
    "    pass\n",
    "\n",
    "\n",
    "# test your code using the following\n",
    "# bias_score(Y_test, P_scores, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4 (10 points)\n",
    "\n",
    "Calculate the false alarm ratio (FAR). FAR is an indicator of what fraction of the predicted events actually did not occur (i.e., they were false alarms). You can calculate the FAR as follows: \n",
    "\n",
    "$ FAR = \\frac{ \\text{false alarms} }{ \\text{hits} + \\text{false alarms} }  $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def far(observation, p_scores, threshold):\n",
    "    #your code goes here\n",
    "    pass\n",
    "\n",
    "\n",
    "# test your code using the following\n",
    "# far(Y_test, P_scores, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5 (10 points)\n",
    "Calculate the threat score. Threat score (which is also referred to as critical success index -- CSI) indicates how well did the predicted event outcomes correspond to the observed events. It measures the fraction of actual __and/or__ predicted events that were correctly predicted. It can be thought of as the accuracy when the correct negatives (TN) have been removed from consideration. It can be calculated as follows:\n",
    "\n",
    "$CSI = \\frac{ \\text{hits} }{ \\text{hits} + \\text{misses} + \\text{false alarms} } $\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csi(observation, p_scores, threshold):\n",
    "    #your code goes here\n",
    "    pass\n",
    "\n",
    "\n",
    "# test your code using the following\n",
    "# csi(Y_test, P_scores, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6 (15 points)\n",
    "Skill scores are often used to understand the improvement of model performance over a given baseline (often a hypothetical, predetermined random forecast result). The first skill score you will implement is _true skill statistic (TSS)_ (which is also known as Hanssen-Kuipers Skill Score. It can be used to understand how well the prediction model separate the events (detected) from the no events. TSS can be calculated as follows:\n",
    "\n",
    "$ TSS = \\frac{\\text{hits} }{\\text{hits} + \\text{misses}} - \\frac{\\text{false alarms} }{\\text{false alarms} + \\text{correct negatives}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tss(observation, p_scores, threshold):\n",
    "    #your code goes here\n",
    "    pass\n",
    "\n",
    "\n",
    "# test your code using the following\n",
    "# tss(Y_test, P_scores, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7 (15 points)\n",
    "The next skill score you will calculate is Gilbert Skill Score (also known as Equitable Threat Score). This is an interesting indicator of performance because it measures the fraction of observed and/or predicted events that were correctly predicted, adjusted for hits (correctly predicted events) associated with random chance. For example, it is easier to correctly predict rain occurrence in a wet climate than in a dry climate. In other words, GSS can answer how well the predicted event occurrences correspond to the observed events (accounting for correct predictions appearing due to chance). The Gilbert Skill Score can be calculated as follows:\n",
    "\n",
    "$GSS = \\frac{ \\text{hits} - \\text{hits}_{random}}{ \\text{hits} + \\text{misses} + \\text{false alarms} - \\text{hits}_{random} } $\n",
    "\n",
    "where the random hits can be calculated as:\n",
    "\n",
    "$ \\text{hits}_{random} = \\frac{ (\\text{hits}+\\text{misses} )* (\\text{hits}+\\text{false alarms} )}{total} $.\n",
    "\n",
    "_total_ is sum of all the elements in confusion matrix. \n",
    "\n",
    "Hint: Notice the similarity between GSS and threat score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gss(observation, p_scores, threshold):\n",
    "    #your code goes here\n",
    "    pass\n",
    "\n",
    "\n",
    "# test your code using the following\n",
    "#gss(Y_test, P_scores, threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Question (10 points)\n",
    "\n",
    "Your last task is to determine an optimal threshold based on prediction scores, observations, and a given performance measure. Create a function called `pick_threshold` which will pick the best prediction score threshold (that will return the highest performance measure value based on the given performance metric). Hint: Python allows you to pass a (performance measure) function (such as `tss`, `csi`, or `f1score`) to `pick_threshold`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_threshold(observation, p_scores, mfunc):\n",
    "    '''Finds the prediction score threshold that returns the highest performance measure value \n",
    "    based on a given measure.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    observation: list\n",
    "        list of observations (1 is there is event, 0 is there is no event)\n",
    "    p_scores: list\n",
    "        list of prediction scores (scores vary between 0 and 1)\n",
    "    mfunc: function\n",
    "        one of the performance measurement functions that will take (observation, p_scores, threshold) as\n",
    "        the list of arguments. (Hint: one can call mfunc(observation, p_scores, threshold) within the \n",
    "        function scope.)\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        threshold value that gives the best performance based on mfunc\n",
    "    \n",
    "    '''\n",
    "    #your code goes here\n",
    "    pass\n",
    "\n",
    "\n",
    "# test your code using the following\n",
    "# pick_threshold(Y_test, P_scores, csi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbaseconda15948ad715344d2b8d9605725bc90444"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}